{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ebe56b50",
   "metadata": {},
   "source": [
    "## Imports and libs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1e3d7dc-71c2-4528-926a-3cbf71a0d23c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import nltk\n",
    "import pprint\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.probability import FreqDist\n",
    "from nltk import ngrams\n",
    "from nltk.stem import RSLPStemmer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.metrics.pairwise import pairwise_distances\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import string\n",
    "import unicodedata\n",
    "import re\n",
    "from wordcloud import WordCloud\n",
    "from pandasql import sqldf\n",
    "import spacy\n",
    "\n",
    "#python -m spacy download pt \n",
    "#nltk.download('stopwords')\n",
    "#nltk.download('punkt')\n",
    "#nltk.download('rslp')\n",
    "# pip install pandasql"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aceded69",
   "metadata": {},
   "source": [
    "## Utils and configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7837f232",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configs\n",
    "\n",
    "reviewsVolume = 20000\n",
    "maxBowSize = 2000\n",
    "commonStopWords = {'pra', 'ca', 'so', 'ja', ''}\n",
    "export_folder = 'results_data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af3f0edb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utils\n",
    "\n",
    "def getJsonFileList():\n",
    "    found_files = []\n",
    "    directory = 'outputs'\n",
    "     \n",
    "    for filename in os.listdir(directory):\n",
    "        if \"reviews-\" in filename and \"-total\" in filename:\n",
    "            found_files.append(filename)\n",
    "    \n",
    "    return found_files\n",
    "\n",
    "def getPersistedReviews():\n",
    "    file_import = f\"outputs/reviews-multi-company-merged-{reviewsVolume}.json\"\n",
    "\n",
    "    with open(file_import, 'r', encoding='utf-8') as arquivo:\n",
    "        data = json.load(arquivo)\n",
    "        return data\n",
    "\n",
    "def getPersistedJson(jsonPath):\n",
    "    with open(jsonPath, 'r', encoding='utf-8') as arquivo:\n",
    "        data = json.load(arquivo)\n",
    "        return data\n",
    "\n",
    "# Remover vírgulas e pontos dos textos\n",
    "def removePunctuationUnique(string):\n",
    "    string = ''.join(c for c in unicodedata.normalize('NFD', string) if unicodedata.category(c) != 'Mn')\n",
    "    string = re.sub(r'[^\\w\\s]', '', string)\n",
    "    return string\n",
    "\n",
    "def removePunctuation(texts):\n",
    "    return [removePunctuationUnique(text) for text in texts]\n",
    "\n",
    "# Tokenizar os textos em palavras\n",
    "def tokenize(texts):\n",
    "    return [word.lower() for text in texts for word in word_tokenize(text, language='portuguese')]\n",
    "    \n",
    "# Remover stopwords\n",
    "def removeStopwords(tokens):\n",
    "    stopWords = set(stopwords.words('portuguese')) \n",
    "    wordsToRemove = stopWords.union(commonStopWords)\n",
    "    return [word for word in tokens if word not in wordsToRemove]\n",
    "\n",
    "def removeStopwordsList(frases):\n",
    "    stopWords = set(stopwords.words('portuguese')) \n",
    "    wordsToRemove = stopWords.union(commonStopWords)\n",
    "    frasesSemStopwords = []\n",
    "    for frase in frases:\n",
    "        tokens = frase.split()  # Dividir a frase em tokens\n",
    "        tokensSemStopwords = [word for word in tokens if word not in wordsToRemove]\n",
    "        novaFrase = ' '.join(tokensSemStopwords)  # Juntar os tokens novamente em uma frase\n",
    "        frasesSemStopwords.append(novaFrase)\n",
    "    return frasesSemStopwords\n",
    "\n",
    "def dataCleaning(texts):\n",
    "   \n",
    "    return tokens\n",
    "\n",
    "# Aplicar stemming nas palavras\n",
    "def applyStemming(tokens):\n",
    "    stemmer = RSLPStemmer()\n",
    "    return [stemmer.stem(word) for word in tokens]\n",
    "\n",
    "# Criar pares consecutivos de tokens\n",
    "def createPairs(words, pairNumber = 2):\n",
    "    less = pairNumber - 1\n",
    "    return [' '.join(words[i:i+pairNumber]) for i in range(len(words)-less)]\n",
    "\n",
    "def generateWordCloud(list, output):\n",
    "    # Criar dicionário de palavras e frequências\n",
    "    palavras_frequencias = {}\n",
    "    for palavra, frequencia in list:\n",
    "        palavras_frequencias[palavra] = frequencia\n",
    "\n",
    "    wordcloud = WordCloud().generate_from_frequencies(palavras_frequencias)\n",
    "    wordcloud.to_file(output)\n",
    "\n",
    "def create_heatmap(similarity, cmap = \"YlGnBu\"):\n",
    "  df = pd.DataFrame(similarity)\n",
    "  df.columns = labels\n",
    "  df.index = labels\n",
    "  fig, ax = plt.subplots(figsize=(5,5))\n",
    "  sns.heatmap(df, cmap=cmap)\n",
    "\n",
    "def generateHorizontalLineGraph(graphdf, output, title, xlabel, ylabel, sortby, kind='barh', width=10, height=6, ascending=True):\n",
    "    by = sortby if sortby else xlabel\n",
    "\n",
    "    #if by != 'NOOP':\n",
    "    graphdf = graphdf.sort_values(by=by, ascending=ascending)\n",
    "\n",
    "    graphdf = graphdf.head(15)\n",
    "    \n",
    "    graph = graphdf.plot(x=ylabel, y=xlabel, kind=kind, figsize=(width, height))\n",
    "    #plt.figure(figsize=(10, 6))\n",
    "    graph.invert_yaxis()\n",
    "    plt.xlabel(xlabel)\n",
    "    plt.ylabel(ylabel)\n",
    "    plt.title(title)\n",
    "    plt.savefig(output)\n",
    "    #plt.tight_layout()\n",
    "    plt.close()\n",
    "\n",
    "    #return graph\n",
    "\n",
    "def getLemmatization(texts):\n",
    "    nlp = spacy.load('pt_core_news_sm', disable=['ner', 'parser'])\n",
    "    total = len(texts)\n",
    "\n",
    "    print(f\"Starting lemmatization from: {total} of texts\")\n",
    "\n",
    "    listpalavras = []\n",
    "    docs = []\n",
    "    count = 0\n",
    "\n",
    "    for text in texts:\n",
    "        current = count + 1\n",
    "        tokens = tokenize([text])\n",
    "        doc = nlp(str([palavra for palavra in tokens]))\n",
    "\n",
    "        lemma = [token.lemma_ for token in doc if token.pos_ != 'PUNCT']\n",
    "\n",
    "        newText = ' '.join(lemma)\n",
    "        listpalavras.append(newText)\n",
    "        docs.append(doc)\n",
    "\n",
    "        print(f\"{current} of {total} texts lemmatized\")\n",
    "        count += 1\n",
    "\n",
    "    return listpalavras, docs\n",
    "\n",
    "def saveStoreJson(fileContent, company, store, fileName):\n",
    "    fileNameBase = f\"{export_folder}/reviews-{company}-{store}-{reviewsVolume}.json\"\n",
    "    file_export = fileName if fileName else fileNameBase\n",
    "\n",
    "    with open(file_export, 'w', encoding='utf-8') as arquivo:\n",
    "        #arquivo.write(json)\n",
    "        json.dump(fileContent, arquivo, indent=4, ensure_ascii=False)\n",
    "\n",
    "def checkIfFileExists(filename):\n",
    "    try:\n",
    "        with open(filename, 'r', encoding='utf-8') as arquivo:\n",
    "            return True\n",
    "    except:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "948c53c0",
   "metadata": {},
   "source": [
    "# Pre work # 1 - Reading reviews JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a417fe3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lendo e filtrando por 2023 \n",
    "\n",
    "jsonData = getPersistedReviews()\n",
    "dfJson = pd.DataFrame(jsonData)\n",
    "dfJson['at'] = pd.to_datetime(dfJson['at'])\n",
    "dfFilteredData = dfJson[dfJson['at'].dt.year == 2023]\n",
    "filteredJsonData = json.loads(dfFilteredData.to_json(orient='records'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cce64203",
   "metadata": {},
   "source": [
    "## Pre work # 1.1 -  Lemmatização\n",
    "###### Entrada: Lista de reviews\n",
    "###### Saída: Lista de reviews com o content lemmatizado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c18bfaa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmPathFile = f\"results/2023-total-volume-{reviewsVolume}-lemmatizated.json\"\n",
    "lemmatizatedDf = pd.DataFrame()\n",
    "lemmExists = checkIfFileExists(lemmPathFile)\n",
    "\n",
    "if lemmExists:\n",
    "    print(\"Loading lemmatizated from a existing file\")\n",
    "    lemmatizatedDf = pd.read_json(lemmPathFile)\n",
    "else:\n",
    "    print(\"Generating new lemmatizated file\")\n",
    "    lemmatizatedDf = dfFilteredData\n",
    "    totalData = lemmatizatedDf.shape[0]\n",
    "    contentList = lemmatizatedDf['content'].tolist()\n",
    "    lemmatizatedContent, _ = getLemmatization(contentList)\n",
    "    lemmatizatedDf[\"content\"] = lemmatizatedContent\n",
    "\n",
    "    print(\"Exporting new lemmatizated file\")\n",
    "    lemmatizatedDf.to_json(f\"results/2023-total-volume-{reviewsVolume}-lemmatizated.json\", orient='records')\n",
    "    lemmatizatedDf.to_csv(f\"results/2023-total-volume-{reviewsVolume}-lemmatizated.csv\", index=False)\n",
    "    lemmatizatedDf.to_excel(f\"results/2023-total-volume-{reviewsVolume}-lemmatizated.xlsx\", index=False)\n",
    "\n",
    "lemmatizatedData = json.loads(lemmatizatedDf.to_json(orient='records'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f943a968",
   "metadata": {},
   "source": [
    "## Pre work # 1.2 - Creating company formats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28d38dd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lendo o arquivo JSON\n",
    "\n",
    "reviewsListPTBRApple = []\n",
    "reviewsListNubankApple = []\n",
    "reviewsListBBApple = []\n",
    "reviewsListItauApple = []\n",
    "reviewsListPTBRGoogle = []\n",
    "reviewsListNubankGoogle = []\n",
    "reviewsListBBGoogle = []\n",
    "reviewsListItauGoogle = []\n",
    "reviewsListNubank = []\n",
    "reviewsListItau = []\n",
    "reviewsListBB = []\n",
    "\n",
    "#for review in lemmatizatedData:\n",
    "for review in filteredJsonData:\n",
    "\n",
    "    content = review['content']\n",
    "\n",
    "    if review['company'] == 'nubank':\n",
    "        reviewsListNubank.append(content)\n",
    "\n",
    "    if review['company'] == 'nubank' and review['store'] == 'Apple':\n",
    "        reviewsListNubankApple.append(content)\n",
    "    \n",
    "    if review['company'] == 'bb':\n",
    "        reviewsListBB.append(content)\n",
    "\n",
    "    if review['company'] == 'bb' and review['store'] == 'Apple':\n",
    "        reviewsListBBApple.append(content)\n",
    "    \n",
    "    if review['company'] == 'itau' and review['store'] == 'Apple':\n",
    "        reviewsListItauApple.append(content)\n",
    "\n",
    "    if review['company'] == 'itau':\n",
    "        reviewsListItau.append(content)\n",
    "\n",
    "    if review['company'] == 'nubank' and review['store'] == 'Google':\n",
    "        reviewsListNubankGoogle.append(content)\n",
    "    \n",
    "    if review['company'] == 'bb' and review['store'] == 'Google':\n",
    "        reviewsListBBGoogle.append(content)\n",
    "    \n",
    "    if review['company'] == 'itau' and review['store'] == 'Google':\n",
    "        reviewsListItauGoogle.append(content)\n",
    "\n",
    "    if review['store'] == 'Apple':\n",
    "        reviewsListPTBRApple.append(content)\n",
    "\n",
    "    if review['store'] == 'Google':\n",
    "        reviewsListPTBRGoogle.append(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2b0d3b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating payload\n",
    "\n",
    "bowPayload = [\n",
    "    {\n",
    "        'company': 'itau',\n",
    "        'reviews': reviewsListItau\n",
    "    },\n",
    "    {\n",
    "        'company': 'nubank',\n",
    "        'reviews': reviewsListNubank\n",
    "    },\n",
    "    {\n",
    "        'company': 'bb',\n",
    "        'reviews': reviewsListBB\n",
    "    },\n",
    "    {\n",
    "        'company': 'nubank-apple',\n",
    "        'reviews': reviewsListNubankApple\n",
    "    },\n",
    "    {\n",
    "        'company': 'bb-apple',\n",
    "        'reviews': reviewsListBBApple\n",
    "    },\n",
    "    {\n",
    "        'company': 'itau-apple',\n",
    "        'reviews': reviewsListItauApple\n",
    "    },\n",
    "    {\n",
    "        'company': 'all-apple',\n",
    "        'reviews': reviewsListPTBRApple\n",
    "    },\n",
    "    {\n",
    "        'company': 'nubank-google',\n",
    "        'reviews': reviewsListNubankGoogle\n",
    "    },\n",
    "    {\n",
    "        'company': 'bb-google',\n",
    "        'reviews': reviewsListBBGoogle\n",
    "    },\n",
    "    {\n",
    "        'company': 'itau-google',\n",
    "        'reviews': reviewsListItauGoogle\n",
    "    },\n",
    "    {\n",
    "        'company': 'all-google',\n",
    "        'reviews': reviewsListPTBRGoogle\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3c95525",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating payload\n",
    "\n",
    "bowPayloadDf = [\n",
    "    {\n",
    "        'company': 'itau',\n",
    "        'reviews': dfFilteredData.loc[dfFilteredData['company'] == 'itau']\n",
    "    },\n",
    "    {\n",
    "        'company': 'nubank',\n",
    "        'reviews': dfFilteredData.loc[dfFilteredData['company'] == 'nubank']\n",
    "    },\n",
    "    {\n",
    "        'company': 'bb',\n",
    "        'reviews': dfFilteredData.loc[dfFilteredData['company'] == 'bb']\n",
    "    },\n",
    "    {\n",
    "        'company': 'nubank-apple',\n",
    "        'reviews': dfFilteredData.loc[(dfFilteredData['company'] == 'nubank') & (dfFilteredData['store'] == 'Apple')]\n",
    "    },\n",
    "    {\n",
    "        'company': 'bb-apple',\n",
    "        'reviews': dfFilteredData.loc[(dfFilteredData['company'] == 'bb') & (dfFilteredData['store'] == 'Apple')]\n",
    "    },\n",
    "    {\n",
    "        'company': 'itau-apple',\n",
    "        'reviews': dfFilteredData.loc[(dfFilteredData['company'] == 'itau') & (dfFilteredData['store'] == 'Apple')]\n",
    "    },\n",
    "    {\n",
    "        'company': 'all-apple',\n",
    "        'reviews': dfFilteredData.loc[dfFilteredData['store'] == 'Apple']\n",
    "    },\n",
    "    {\n",
    "        'company': 'nubank-google',\n",
    "        'reviews': dfFilteredData.loc[(dfFilteredData['company'] == 'nubank') & (dfFilteredData['store'] == 'Google')]\n",
    "    },\n",
    "    {\n",
    "        'company': 'bb-google',\n",
    "        'reviews': dfFilteredData.loc[(dfFilteredData['company'] == 'bb') & (dfFilteredData['store'] == 'Google')]\n",
    "    },\n",
    "    {\n",
    "        'company': 'itau-google',\n",
    "        'reviews': dfFilteredData.loc[(dfFilteredData['company'] == 'itau') & (dfFilteredData['store'] == 'Google')]\n",
    "    },\n",
    "    {\n",
    "        'company': 'all-google',\n",
    "        'reviews': dfFilteredData.loc[dfFilteredData['store'] == 'Google']\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fd619e0",
   "metadata": {},
   "source": [
    "# Output # 1 - Lista de comentários por bancos\n",
    "###### Entrada: Lista (JSON) `reviews-multi-company-merged.json` \n",
    "###### Saída: Arquivo XLSX com todos os reviews filtrados por 2023."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8abc666",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a pandas dataframe\n",
    "#df = pd.DataFrame(json_data)\n",
    "#df = df[['company', 'store', 'content']]\n",
    "# Export the dataframe to xlsx\n",
    "dfFilteredData.to_excel(f\"results/2023-total-volume-{reviewsVolume}.xlsx\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5c6cf58",
   "metadata": {},
   "source": [
    "## Output # 1.2 - Lista de comentários por bancos filtradas\n",
    "###### Entrada: Lista (JSON) `reviews-multi-company-merged.json` \n",
    "###### Saída: Arquivo XLSX com todos os reviews filtrados por 2023 e stop words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be9ab796",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Output # 1.2 - "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70d020f1",
   "metadata": {},
   "source": [
    "# Output # 2 -  BoW\n",
    "###### Entrada: Lista de reviews por empresa: `reviewsListPTBR`, `reviewsListNubank`, `reviewsListBB`, `reviewsListItau`\n",
    "###### Saída: Lista de BoW de 1, 2, 3, 4 e 5 index por empresa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "980a35fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "listOfFreqDist = []\n",
    "listOfIndex = [1,2,3,4,5]\n",
    "\n",
    "def getListOfFrequency(texts, index):\n",
    "    texts = removePunctuation(texts)\n",
    "    words = tokenize(texts)\n",
    "    words = removeStopwords(words)\n",
    "    #words = applyStemming(words)\n",
    "    pairs = createPairs(words, index)\n",
    "    # Calcular a distribuição de frequência das palavras\n",
    "    freq_dist = FreqDist(pairs)\n",
    "    most_common_words = freq_dist.most_common()\n",
    "    return most_common_words\n",
    "\n",
    "def iterateListOfIndex(texts, company):\n",
    "    acc = []\n",
    "    for index in listOfIndex:\n",
    "        listResult = getListOfFrequency(texts, index)\n",
    "        acc.append({ 'index': index, 'list': listResult, 'company': company })\n",
    "\n",
    "    return acc\n",
    "\n",
    "for item in bowPayload:\n",
    "    texts = item['reviews'] \n",
    "    company = item['company']\n",
    "    frequencies = iterateListOfIndex(texts, company)\n",
    "    listOfFreqDist = listOfFreqDist + frequencies\n",
    "    #df_freq_dist = pd.DataFrame(frequencies, columns=['Word', 'Frequency'])\n",
    "    #df_freq_dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96192d92",
   "metadata": {},
   "outputs": [],
   "source": [
    "for item in listOfFreqDist:\n",
    "  company = item['company']\n",
    "  index = item['index']\n",
    "  itemList = item['list']\n",
    "\n",
    "  #pprint(f\"Getting {company} index {index} results\")\n",
    "  df = pd.DataFrame(itemList, columns=['Word', 'Frequency'])\n",
    "  df = df.drop(df.index[maxBowSize:])\n",
    "  \n",
    "  print(f\"Creating {company} bow index {index} excel file\")\n",
    "  df.to_excel(f\"results/{company}/bow-result-index-{index}.xlsx\", index=False)\n",
    "\n",
    "  print(f\"Creating {company} bow wordcloud index {index}\")\n",
    "  generateWordCloud(itemList, f\"results/{company}/wordcloud-index-{index}.png\")\n",
    "\n",
    "  print(f\"Creating {company} index {index} graph file\")\n",
    "  generateHorizontalLineGraph(\n",
    "    df, \n",
    "    f\"results/{company}/bow-graph-index-{index}.png\", \n",
    "    f\"BoW {company} index {index}\", \n",
    "    \"Frequency\", \n",
    "    \"Word\",\n",
    "    width = 10 + (index * 4),\n",
    "    height = 6, \n",
    "    sortby = \"Frequency\",\n",
    "    ascending = False\n",
    "  )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58c24c19",
   "metadata": {},
   "source": [
    "# Output # 2.1 -  BoW por Score\n",
    "###### Entrada: Lista de reviews\n",
    "###### Saída: Lista de BoW de 1, 2, 3, 4 e 5 index por score por empresa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c4d0a34",
   "metadata": {},
   "outputs": [],
   "source": [
    "listOfIndex = [1,2,3,4,5]\n",
    "\n",
    "def getListOfFrequency(texts, index):\n",
    "    texts = removePunctuation(texts)\n",
    "    words = tokenize(texts)\n",
    "    words = removeStopwords(words)\n",
    "    #words = applyStemming(words)\n",
    "    pairs = createPairs(words, index)\n",
    "    # Calcular a distribuição de frequência das palavras\n",
    "    freq_dist = FreqDist(pairs)\n",
    "    most_common_words = freq_dist.most_common()\n",
    "    return most_common_words\n",
    "\n",
    "def generateInfos(texts, company, index, score):\n",
    "   #pprint(f\"Getting {company} index {index} results\")\n",
    "    dfinfo = pd.DataFrame(texts, columns=['Word', 'Frequency'])\n",
    "    dfinfo = dfinfo.drop(df.index[maxBowSize:])\n",
    "    \n",
    "    print(f\"Creating {company} bow index {index} excel file for score {score}\")\n",
    "    dfinfo.to_excel(f\"results/{company}/score/{score}/score-bow-result-index-{index}.xlsx\", index=False)\n",
    "\n",
    "    print(f\"Creating {company} bow wordcloud index {index} for score {score}\")\n",
    "    generateWordCloud(texts, f\"results/{company}/score/{score}/score-wordcloud-index-{index}.png\")\n",
    "\n",
    "    print(f\"Creating {company} index {index} graph file for score {score}\")\n",
    "    generateHorizontalLineGraph(\n",
    "        dfinfo, \n",
    "        f\"results/{company}/score/{score}/score-bow-graph-index-{index}.png\", \n",
    "        f\"BoW {company} index {index}\", \n",
    "        \"Frequency\", \n",
    "        \"Word\",\n",
    "        width = 10 + (index * 4),\n",
    "        height = 6, \n",
    "        sortby = \"Frequency\",\n",
    "        ascending = False\n",
    "    )\n",
    "\n",
    "def iterateListOfIndex(texts, company, score):\n",
    "    for index in listOfIndex:\n",
    "        listResult = getListOfFrequency(texts, index)\n",
    "        generateInfos(listResult, company, index, score)\n",
    "\n",
    "def iterateScores(item):\n",
    "    reviews = item['reviews']\n",
    "    company = item['company']\n",
    "    reviews = pd.DataFrame(reviews)\n",
    "\n",
    "    scoreList = {\n",
    "        '1': reviews.loc[reviews['score'] == 1],\n",
    "        '2': reviews.loc[reviews['score'] == 2],\n",
    "        '3': reviews.loc[reviews['score'] == 3],\n",
    "        '4': reviews.loc[reviews['score'] == 4],\n",
    "        '5': reviews.loc[reviews['score'] == 5]\n",
    "    }\n",
    "\n",
    "    for score, scoreItem in scoreList.items():\n",
    "        texts = scoreItem['content'].tolist()\n",
    "        iterateListOfIndex(texts, company, score)\n",
    "\n",
    "for item in bowPayloadDf:\n",
    "    iterateScores(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6024da3",
   "metadata": {},
   "outputs": [],
   "source": [
    "for item in listOfFreqDist:\n",
    "  company = item['company']\n",
    "  index = item['index']\n",
    "  itemList = item['list']\n",
    "\n",
    "  #pprint(f\"Getting {company} index {index} results\")\n",
    "  df = pd.DataFrame(itemList, columns=['Word', 'Frequency'])\n",
    "  df = df.drop(df.index[maxBowSize:])\n",
    "  \n",
    "  print(f\"Creating {company} bow index {index} excel file\")\n",
    "  df.to_excel(f\"results/{company}/bow-result-index-{index}.xlsx\", index=False)\n",
    "\n",
    "  print(f\"Creating {company} bow wordcloud index {index}\")\n",
    "  generateWordCloud(itemList, f\"results/{company}/wordcloud-index-{index}.png\")\n",
    "\n",
    "  print(f\"Creating {company} index {index} graph file\")\n",
    "  generateHorizontalLineGraph(\n",
    "    df, \n",
    "    f\"results/{company}/bow-graph-index-{index}.png\", \n",
    "    f\"BoW {company} index {index}\", \n",
    "    \"Frequency\", \n",
    "    \"Word\",\n",
    "    width = 10 + (index * 4),\n",
    "    height = 6, \n",
    "    sortby = \"Frequency\",\n",
    "    ascending = False\n",
    "  )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae39508e",
   "metadata": {},
   "source": [
    "# Output # 3 -  N-GRAM \n",
    "###### Entrada: Lista de reviews por empresa: `reviewsListPTBR`, `reviewsListNubank`, `reviewsListBB`, `reviewsListItau`\n",
    "###### Saída: Lista de BoW de 1, 2, 3, 4 e 5 index por empresa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c9085ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "testedf = pd.DataFrame(bowPayload)\n",
    "testedf.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3afcbede",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform list of objetc and list to list of object\n",
    "\n",
    "newList = []\n",
    "\n",
    "def wordsToObject(words, company, index):\n",
    "  for word in words:\n",
    "    store = company.split('-')[1]\n",
    "    companyName = company.split('-')[0]\n",
    "    newList.append({\n",
    "      'id': index,\n",
    "      'company': companyName,\n",
    "      'store': store,\n",
    "      'word': word\n",
    "    })\n",
    "\n",
    "def enumerateList(obj):\n",
    "  currentCompany = obj['company']\n",
    "  currentReviews = obj['reviews']\n",
    "  pprint.pprint(f\"Company: {currentCompany}\")\n",
    "\n",
    "  for index, review in enumerate(currentReviews):\n",
    "    texts = removePunctuation([review])\n",
    "    words = tokenize(texts)\n",
    "    words = removeStopwords(words)\n",
    "    pairs = createPairs(words, 1)\n",
    "    wordsToObject(pairs, currentCompany, index)\n",
    "\n",
    "  return newList\n",
    "\n",
    "def transformListToObjectList(obj):\n",
    "  for item in obj:\n",
    "    enumerateList(item)\n",
    "\n",
    "transformListToObjectList(bowPayload)\n",
    "  \n",
    "df = pd.DataFrame(newList)\n",
    "df.to_csv(f\"results/total-words-{reviewsVolume}.csv\", index=False)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e7b5856",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform list of objetc and list to list of object\n",
    "\n",
    "newList = []\n",
    "\n",
    "def wordsToObject(words, company, index):\n",
    "  for word in words:\n",
    "    store = company.split('-')[1]\n",
    "    companyName = company.split('-')[0]\n",
    "    newList.append({\n",
    "      'id': index,\n",
    "      'company': companyName,\n",
    "      'store': store,\n",
    "      'word': word\n",
    "    })\n",
    "\n",
    "def enumerateList(obj):\n",
    "  currentCompany = obj['company']\n",
    "  currentReviews = obj['reviews']\n",
    "  pprint.pprint(f\"Company: {currentCompany}\")\n",
    "\n",
    "  for index, review in enumerate(currentReviews):\n",
    "    texts = removePunctuation([review])\n",
    "    words = tokenize(texts)\n",
    "    words = removeStopwords(words)\n",
    "    pairs = createPairs(words, 1)\n",
    "    wordsToObject(pairs, currentCompany, index)\n",
    "\n",
    "  return newList\n",
    "\n",
    "def transformListToObjectList(obj):\n",
    "  for item in obj:\n",
    "    enumerateList(item)\n",
    "\n",
    "transformListToObjectList(bowPayload)\n",
    "  \n",
    "df = pd.DataFrame(newList)\n",
    "df.to_csv(f\"results/total-words-{reviewsVolume}.csv\", index=False)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc962cc2",
   "metadata": {},
   "source": [
    "# Output # 4 - TF-IDF em diferentes indexs por bancos\n",
    "###### Entrada: Lista de reviews por empresa: `reviewsListPTBR`, `reviewsListNubank`, `reviewsListBB`, `reviewsListItau`\n",
    "###### Saída: Lista de TF-IDF de 1, 2, 3, 4 e 5 index por empresa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4d5f2dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "listOfIndex = [1,2,3,4,5]\n",
    "\n",
    "def calculate_tfidf(texts, index):\n",
    "    \n",
    "    textsCleaned = removePunctuation(texts)\n",
    "    textsWithoutStopwords = removeStopwordsList(textsCleaned)\n",
    "    \n",
    "    # Calcular o TF-IDF dos termos\n",
    "    vectorizer = TfidfVectorizer(ngram_range=(index, index))\n",
    "    tfidf_matrix = vectorizer.fit_transform(textsWithoutStopwords)\n",
    "\n",
    "    # Obter os termos e seus respectivos valores de TF-IDF\n",
    "    feature_names = vectorizer.get_feature_names_out()\n",
    "    tfidf_values = tfidf_matrix.toarray()[0]\n",
    "\n",
    "    tfidf_dict = {term: tfidf for term, tfidf in zip(feature_names, tfidf_values)}\n",
    "    tfidf_df = pd.DataFrame(tfidf_dict.items(), columns=['Term', 'TF-IDF'])\n",
    "    tfidf_df = tfidf_df.sort_values(by='TF-IDF', ascending=False)\n",
    "    tfidf_df = tfidf_df[tfidf_df['TF-IDF'] > 0]\n",
    "\n",
    "    return tfidf_df\n",
    "\n",
    "def iterateListOfIndex(texts, company):\n",
    "    \n",
    "    for index in listOfIndex:\n",
    "        print(f\"Getting {company} index {index} results\")\n",
    "        itemTFIDF = calculate_tfidf(texts, index)\n",
    "        print(f\"Creating {company} index {index} excel file\")\n",
    "        itemTFIDF.to_excel(f\"results/{company}/tfidf-index-{index}.xlsx\", index=False)\n",
    "        print(f\"Creating {company} index {index} wordcloud file\")\n",
    "        itemList = itemTFIDF.values.tolist()\n",
    "        generateWordCloud(itemList, f\"results/{company}/tfidf-wordcloud-index-{index}.png\")\n",
    "        print(f\"Creating {company} index {index} graph file\")\n",
    "        generateHorizontalLineGraph(\n",
    "            itemTFIDF, \n",
    "            f\"results/{company}/tfidf-graph-index-{index}.png\", \n",
    "            f\"TF-IDF {company} index {index}\", \n",
    "            \"TF-IDF\", \n",
    "            \"Term\",\n",
    "            width = 10 + (index * 4),\n",
    "            height = 6,\n",
    "            sortby = \"TF-IDF\",\n",
    "            ascending = False\n",
    "        )\n",
    "\n",
    "for item in bowPayload:\n",
    "    texts = item['reviews'] \n",
    "    company = item['company']\n",
    "    iterateListOfIndex(texts, company)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
